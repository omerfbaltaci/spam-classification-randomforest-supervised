{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":260807,"sourceType":"datasetVersion","datasetId":109196}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-23T19:31:44.833225Z","iopub.execute_input":"2024-09-23T19:31:44.833775Z","iopub.status.idle":"2024-09-23T19:31:44.873816Z","shell.execute_reply.started":"2024-09-23T19:31:44.833726Z","shell.execute_reply":"2024-09-23T19:31:44.872456Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import string\n\nimport re\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib as plt\n\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:31:46.364172Z","iopub.execute_input":"2024-09-23T19:31:46.365254Z","iopub.status.idle":"2024-09-23T19:31:47.842557Z","shell.execute_reply.started":"2024-09-23T19:31:46.365193Z","shell.execute_reply":"2024-09-23T19:31:47.841198Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Essential Library Imports for Email Classification\n\nTo effectively preprocess the email text data and train our classification model, we need to import several important Python libraries:\n\n- **string**: Provides access to a list of common punctuation characters, which will be useful for filtering punctuation from the text.\n- **re**: Enables the use of regular expressions for pattern matching, which is essential for text cleaning tasks such as punctuation removal.\n- **pandas**: A powerful library for data manipulation and analysis, which allows us to handle our dataset as a DataFrame.\n- **numpy**: Provides support for large, multi-dimensional arrays and matrices, and includes mathematical functions to operate on these arrays.\n- **matplotlib**: A popular data visualization library, which we'll use to create visual representations of our data.\n- **nltk**: The Natural Language Toolkit, which contains tools like stopwords and stemmers to process and simplify text data.\n- **sklearn**: The Scikit-learn library, essential for machine learning tasks. We use `train_test_split` to divide the dataset into training and testing sets.\n\nThese libraries form the foundation of our spam classification project and will assist in various tasks like text preprocessing, data handling, and model training. Below is the code to import them:\n\n---","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/spam-mails-dataset/spam_ham_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:31:49.880531Z","iopub.execute_input":"2024-09-23T19:31:49.881219Z","iopub.status.idle":"2024-09-23T19:31:50.029621Z","shell.execute_reply.started":"2024-09-23T19:31:49.881169Z","shell.execute_reply":"2024-09-23T19:31:50.028524Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Loading the Spam/Ham Dataset\n\nIn this step, we will load the **Spam/Ham Dataset** using the `pandas` library. This dataset contains labeled email data, where each email is classified as either \"spam\" or \"ham\" (non-spam). The dataset will be used to train a machine learning model that can automatically classify emails based on their content.\n\nThe file is read into a **DataFrame** using `pd.read_csv()`, allowing us to efficiently manipulate, preprocess, and analyze the data in a tabular format. Once loaded, we can begin to clean and prepare the text data for model training.\n\n---","metadata":{}},{"cell_type":"code","source":"df.info()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:31:51.718540Z","iopub.execute_input":"2024-09-23T19:31:51.718952Z","iopub.status.idle":"2024-09-23T19:31:51.765003Z","shell.execute_reply.started":"2024-09-23T19:31:51.718911Z","shell.execute_reply":"2024-09-23T19:31:51.763874Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5171 entries, 0 to 5170\nData columns (total 4 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Unnamed: 0  5171 non-null   int64 \n 1   label       5171 non-null   object\n 2   text        5171 non-null   object\n 3   label_num   5171 non-null   int64 \ndtypes: int64(2), object(2)\nmemory usage: 161.7+ KB\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0 label                                               text  \\\n0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n\n   label_num  \n0          0  \n1          0  \n2          0  \n3          1  \n4          0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>label</th>\n      <th>text</th>\n      <th>label_num</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>605</td>\n      <td>ham</td>\n      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2349</td>\n      <td>ham</td>\n      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3624</td>\n      <td>ham</td>\n      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4685</td>\n      <td>spam</td>\n      <td>Subject: photoshop , windows , office . cheap ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2030</td>\n      <td>ham</td>\n      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Exploring the Dataset: Structure and First Look\n\nBefore we begin processing the data, it's important to understand the structure and contents of our **Spam/Ham Dataset**. The following steps will help us get an overview of the data:\n\n- **`df.info()`**: This command provides essential information about the dataset, including the number of entries, column names, data types, and whether there are any missing values.\n  \n- **`df.head()`**: This function allows us to preview the first five rows of the dataset, giving us an initial glimpse into the data and its structure. We'll use this to check the content of the emails and their corresponding labels (spam or ham).\n\nThese initial exploratory steps are crucial for ensuring that the dataset is properly loaded and formatted before we proceed with preprocessing and model training.\n\n---","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:31:53.327415Z","iopub.execute_input":"2024-09-23T19:31:53.327908Z","iopub.status.idle":"2024-09-23T19:31:53.410938Z","shell.execute_reply.started":"2024-09-23T19:31:53.327861Z","shell.execute_reply":"2024-09-23T19:31:53.409740Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Initializing Text Preprocessing Tools\n\nIn this step, we will initialize key tools from the **Natural Language Toolkit (NLTK)** to help us preprocess the email text data:\n\n- **PorterStemmer**: A stemming algorithm that reduces words to their root form. For example, words like \"running\", \"runner\", and \"runs\" will be reduced to \"run\". This simplifies the dataset and reduces redundancy in word forms.\n  \n- **Stopwords**: Stopwords are common words like \"the\", \"is\", \"in\", and \"and\", which do not contribute much to the meaning of the text. We'll download the list of English stopwords from NLTK and remove them from the email content during preprocessing to focus on the more significant words.\n\nThese tools are essential for cleaning and simplifying the text, which will improve the accuracy of our machine learning model. Here’s the code to initialize them:\n\n---","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    \n    text = text.lower()\n    text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n    \n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n    \n    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n    \n    return \" \".join(stemmed_words)\n\ndf['simplified_text'] = df['text'].apply(preprocess_text)\n\nprint(df[['text', 'simplified_text']].head(3))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:31:55.041527Z","iopub.execute_input":"2024-09-23T19:31:55.042051Z","iopub.status.idle":"2024-09-23T19:32:15.153268Z","shell.execute_reply.started":"2024-09-23T19:31:55.042002Z","shell.execute_reply":"2024-09-23T19:32:15.151850Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"                                                text  \\\n0  Subject: enron methanol ; meter # : 988291\\r\\n...   \n1  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n2  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n\n                                     simplified_text  \n0  subject enron methanol meter 988291 follow not...  \n1  subject hpl nom januari 9 2001 see attach file...  \n2  subject neon retreat ho ho ho around wonder ti...  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Preprocessing Email Text Data for Spam Classification\n\nBefore training our machine learning model to classify emails as spam or not, we need to preprocess the text data to improve its quality and ensure the model performs well. The preprocessing steps include:\n\n1. **Lowercasing**: Convert all text in the email to lowercase to maintain uniformity and avoid case-sensitive mismatches.\n2. **Removing Punctuation**: Punctuation marks do not contribute meaningfully to the spam classification task, so they will be removed using regular expressions.\n3. **Tokenization and Stopword Removal**: The text will be split into individual words (tokens), and common English stopwords (e.g., \"the\", \"is\", \"and\") will be removed to focus on more meaningful words.\n4. **Stemming**: Words will be reduced to their root form using the Porter Stemmer (e.g., \"running\" becomes \"run\"), which reduces the dimensionality of the feature space.\n\nBy applying these preprocessing steps, we aim to simplify and clean the data, making it more suitable for machine learning algorithms. The code below applies these transformations to the `text` column of our dataset, creating a new column `simplified_text` with the processed text.\n\n---","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['simplified_text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:32:25.011880Z","iopub.execute_input":"2024-09-23T19:32:25.013159Z","iopub.status.idle":"2024-09-23T19:32:25.865691Z","shell.execute_reply.started":"2024-09-23T19:32:25.013092Z","shell.execute_reply":"2024-09-23T19:32:25.864720Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Vectorization of Text Data\n\nIn this step, we're using `TfidfVectorizer` from the `sklearn.feature_extraction.text` module to convert the textual data into numerical features. TF-IDF (Term Frequency-Inverse Document Frequency) helps in weighing the importance of words relative to each document in the dataset.\n\n- **`TfidfVectorizer()`**: Initializes the TF-IDF vectorizer.\n- **`fit_transform(df['simplified_text'])`**: \n  - **`fit`**: Learns the vocabulary from the 'simplified_text' column of our dataframe.\n  - **`transform`**: Converts the text into a TF-IDF-weighted term-document matrix.\n\nThe result, `X`, is a sparse matrix where each row represents a document and each column represents a unique word from the corpus, with the corresponding values being their TF-IDF scores.\n\n---","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:55:00.273329Z","iopub.execute_input":"2024-09-23T19:55:00.273830Z","iopub.status.idle":"2024-09-23T19:55:00.287674Z","shell.execute_reply.started":"2024-09-23T19:55:00.273782Z","shell.execute_reply":"2024-09-23T19:55:00.286283Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the Dataset into Training and Testing Sets\n\nIn this step, we split our dataset into training and testing subsets using `train_test_split` from the `sklearn.model_selection` module. This ensures we have separate data for training the model and for evaluating its performance.\n\n- **`train_test_split(X, df['label'], test_size=0.2, random_state=42)`**:\n  - **`X`**: The TF-IDF matrix representing the features (transformed text data).\n  - **`df['label']`**: The target labels associated with each document (spam or not spam).\n  - **`test_size=0.2`**: 20% of the data will be used for testing, and 80% for training.\n  - **`random_state=42`**: Sets a seed for reproducibility so that the same random splitting can be achieved every time the code is run.\n\nThe output variables:\n- **`X_train`**: Training set features (80% of the TF-IDF data).\n- **`X_test`**: Testing set features (20% of the TF-IDF data).\n- **`y_train`**: Training set labels corresponding to `X_train`.\n- **`y_test`**: Testing set labels corresponding to `X_test`.\n\n---","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:55:03.104534Z","iopub.execute_input":"2024-09-23T19:55:03.105138Z","iopub.status.idle":"2024-09-23T19:55:09.232364Z","shell.execute_reply.started":"2024-09-23T19:55:03.105077Z","shell.execute_reply":"2024-09-23T19:55:09.231184Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Training the Random Forest Classifier\n\nIn this step, we are using a Random Forest Classifier to train our model on the training data. Random Forest is an ensemble learning method that creates multiple decision trees during training and outputs the class that is the mode of the classes predicted by individual trees.\n\n- **`RandomForestClassifier()`**: Initializes the Random Forest Classifier.\n- **`fit(X_train, y_train)`**: \n  - **`X_train`**: The training features (TF-IDF matrix for the training data).\n  - **`y_train`**: The training labels (whether the emails are spam or not).\n\nThe model learns patterns in the training data to make predictions on unseen data in the future.\n\n---","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:55:30.251153Z","iopub.execute_input":"2024-09-23T19:55:30.252158Z","iopub.status.idle":"2024-09-23T19:55:30.397473Z","shell.execute_reply.started":"2024-09-23T19:55:30.252099Z","shell.execute_reply":"2024-09-23T19:55:30.396324Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         ham       0.99      0.98      0.99       742\n        spam       0.96      0.97      0.96       293\n\n    accuracy                           0.98      1035\n   macro avg       0.97      0.98      0.98      1035\nweighted avg       0.98      0.98      0.98      1035\n\nAccuracy: 0.9797101449275363\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Evaluating the Model Performance (Accuracy: %97.97101)\n\nAfter training the Random Forest Classifier, we now evaluate its performance on the test data using various metrics:\n\n- **`model.predict(X_test)`**: Generates predictions (`y_pred`) for the test set based on the patterns learned from the training data.\n  \n- **`classification_report(y_test, y_pred)`**: Provides detailed performance metrics including:\n  - **Precision**: The proportion of correctly predicted positive observations out of all predicted positives.\n  - **Recall (Sensitivity)**: The proportion of correctly predicted positive observations out of all actual positives.\n  - **F1-Score**: The harmonic mean of precision and recall, giving a single performance score for each class.\n  - **Support**: The number of occurrences of each class in the true labels.\n\n- **`accuracy_score(y_test, y_pred)`**: Calculates the overall accuracy of the model, which is the proportion of correctly predicted labels out of the total test samples.\n\nThis evaluation helps in understanding the model’s performance in terms of both accuracy and how well it balances precision and recall for each class.\n\n---","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30]}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_params_)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T19:56:07.753957Z","iopub.execute_input":"2024-09-23T19:56:07.754559Z","iopub.status.idle":"2024-09-23T20:01:14.216609Z","shell.execute_reply.started":"2024-09-23T19:56:07.754500Z","shell.execute_reply":"2024-09-23T20:01:14.215276Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"{'max_depth': None, 'n_estimators': 100}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Hyperparameter Tuning with Grid Search\n\nIn this step, we use **GridSearchCV** to perform an exhaustive search over specified hyperparameters for the Random Forest Classifier, optimizing the model's performance.\n\n- **`param_grid`**: A dictionary defining the hyperparameters to be tested:\n  - **`n_estimators`**: Number of trees in the forest (100, 200, or 300).\n  - **`max_depth`**: The maximum depth of each tree (None, 10, 20, or 30). `None` allows nodes to expand until all leaves are pure or contain fewer than the minimum number of samples required to split.\n\n- **`GridSearchCV()`**: \n  - Initializes a grid search with the Random Forest Classifier.\n  - **`cv=5`**: Performs 5-fold cross-validation, splitting the training set into 5 subsets and training the model on different combinations to validate performance.\n\n- **`fit(X_train, y_train)`**: Trains multiple models with different combinations of the specified hyperparameters on the training data.\n\n- **`grid_search.best_params_`**: Outputs the best combination of hyperparameters after the search.\n\nThis method helps in finding the most effective set of parameters to improve model performance.\n\n---","metadata":{}},{"cell_type":"code","source":"optimized_model = RandomForestClassifier(max_depth=None, n_estimators=100)\n\noptimized_model.fit(X_train, y_train)\n\ny_pred_optimized = optimized_model.predict(X_test)\n\nprint(classification_report(y_test, y_pred_optimized))\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_optimized))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T20:04:46.918503Z","iopub.execute_input":"2024-09-23T20:04:46.919413Z","iopub.status.idle":"2024-09-23T20:04:53.009496Z","shell.execute_reply.started":"2024-09-23T20:04:46.919358Z","shell.execute_reply":"2024-09-23T20:04:53.008253Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         ham       0.99      0.99      0.99       742\n        spam       0.96      0.98      0.97       293\n\n    accuracy                           0.98      1035\n   macro avg       0.98      0.98      0.98      1035\nweighted avg       0.98      0.98      0.98      1035\n\nAccuracy: 0.9835748792270531\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training and Evaluating the Optimized Model (Accuracy: %98.35748)\n\nAfter identifying the optimal hyperparameters through Grid Search, we train a new Random Forest model with the best parameters and evaluate its performance.\n\n- **`RandomForestClassifier(max_depth=None, n_estimators=100)`**: \n  - Initializes the Random Forest Classifier with the best parameters found from the Grid Search.\n  - **`max_depth=None`**: Allows the trees to expand fully.\n  - **`n_estimators=100`**: Specifies the number of trees in the forest as 100.\n\n- **`fit(X_train, y_train)`**: Trains the optimized model on the training data.\n\n- **`y_pred_optimized = optimized_model.predict(X_test)`**: Predicts the labels for the test set using the optimized model.\n\n- **`classification_report(y_test, y_pred_optimized)`**: Displays precision, recall, F1-score, and support for each class (spam or not spam).\n\n- **`accuracy_score(y_test, y_pred_optimized)`**: Calculates the overall accuracy of the optimized model.\n\nThis step helps evaluate whether the hyperparameter tuning has improved the model’s performance compared to the default configuration.\n\n---","metadata":{}}]}
